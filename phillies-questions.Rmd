---
title: "Predicting K% in the Second Half of 2017"
author: "Ryan Pollack"
date: "October 2018"
output: html_document
---

# What is this Analysis?
This project predicts second-half strikeout rates for a select group of pitchers in 2017. It's in response to the questionnaire located at https://questionnaire-148920.appspot.com/qa/.

# Why is it Important?
Going into the second half of each season, GM's have to decide whom to target in trade, whom to put onto their own trading block, and whom to keep on the roster. Predicting second-half strikeout rates can aid all of these decisions by identifying pitchers whom other teams might undervalue and therefore should be sought in a trade, whom other teams might overvalue and therefore should be offered up in trade, and who can help the current roster succeed.

# Success Metrics
How will we know these predictions are successful? I'll evaluate the predictions in four ways: the scatterplot of predicted vs. actual values, the R^2 of the best-fit line on this scatterplot, the scatterplot of the predicted values vs. residuals, and the RMSE of the predicted vs. actual values.

This project uses RMSE instead of mean absolute error (MAE) because RMSE penalizes large errors more than MAE does. In the world of baseball where roster spots are limited, and especially in forecasting second-half results in order to drive trade decisions, large misses in predicted K% can have outsized effects.

# Analysis
The following sections are the R code that set up the analysis, get the data, train the model, predict results, and evaluate the predictions.

## Setup
The following code sets up the libraries and ggplot themes used:

```{r setup, echo = TRUE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# load required libraries
library(tidyverse)
library(xgboost)
library(caret)

# define plot aesthetics
fgt = theme(
  panel.background = element_rect(color = "lightgrey", fill = "white"),
  axis.title = element_text(family = "Lato"),
  axis.text = element_text(family = "Lato"),
  legend.title = element_text(family = "Lato"),
  legend.text = element_text(family = "Lato"),
  legend.background = element_rect(fill = "white"),
  legend.key = element_rect(fill = "white"),
  plot.title = element_text(family = "Lato"),
  panel.grid.major = element_line(size = .1, color = "lightgrey"),
  panel.grid.minor = element_line(size = 0),
  strip.text = element_text(family="Lato")
)

```

## Get Data
In addition to the data contained in strikeouts.csv, I incorporated the throwing hand of each pitcher. So much of batter/pitcher matchups involves platoon splits that I wanted to see if that information helped the model. I also included the pitcher's strikeout rate in 2016. Strikeout rate is among the most stable of pitching metrics; it correlates year-to-year very well, so I thought it might help second-half 2017 predictions. 

Both additional datasets proved useful in constructing the model.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# get data from the first half of 2017
# source: https://questionnaire-148920.appspot.com/qa/
pitcher_data <- read_csv("~/Downloads/strikeouts.csv")

# get throwing hand of pitchers. source: fangraphs
pitcher_handedness <- read_csv("pitcher_handedness.csv")

# get strikeout rates from 2016. source: fangraphs
pitcher_data_2016 <- read_csv("2016-strikeout-rates.csv")

# combine datasets into one tibble
pitchers_2017 <- inner_join(pitcher_data, pitcher_handedness, by=c("fangraphs_id" = "PlayerId")) %>%
  inner_join(pitcher_data_2016, by=c("fangraphs_id" = "PlayerId"))

# create training dataset by removing unnecessary features
pitchers_2017_train <- 
  pitchers_2017 %>% 
  select(-fangraphs_id, -`2ndHalfIP`, -Name, -Team)
```

## Choose the Algorithm
I chose the xgboost algorithm because of its reputation as a fast, accurate algorithm that handles collinearity well. This latter feature means I can focus less on which features to include and more on tuning the model and evaluating the results.

For example, FB% and GB% are mutually exclusive and so are strongly correlated (r = -0.916 using Spearman correlation). With another algorithm I'd have to choose one or the other. The xgboost algorithm helps ensure I can include both without worrying about overfitting. 

For reference, the following figure shows a correlation plot of all features I used except for throwing hand:

```{r correlation matrix, echo=TRUE, fig.align='center'}

# compute and display correlation matrix for features.
# remove features we don't care about, compute its correlation matrix, add in row names because
# converting the matrix to a tibble removes them, tidy, and plot
pitchers_2017_train %>%
  select(-Throws, -`2ndHalfK%`) %>%
  cor(method="spearman") %>%
  as_tibble() %>%
  bind_cols(pitchers_2017_train %>% 
              select(-Throws, -`2ndHalfK%`) %>%
              names() %>%
              as_tibble()) %>%
  select(metric = value, everything()) %>%
  gather(other_metric, r, -metric) %>%
  ggplot(aes(metric, other_metric, color = r, fill = r)) + geom_tile() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(x="", y="", caption = "* Spearman") + ggtitle("Correlation* Matrix of Features")
```

## Tune and Train the Model
I used the caret package to tune and train the xgboost model. I defined a set of tuning parameters for the xgboost algorithm itself, performed 10-fold cross-validation for each combination of parameters, and selected the model that minimized the RMSE of the predictions. The caret package also handles encoding the categorical "Throws" feature. 

This process is iterative in itself, but choosing the parameters for `parameter_grid` is iterative too. The `nrounds` (the number of decision trees to use) and `max_depth` (how deep each tree can go) parameters are important, so I experiemented with several option sets for each. After each training run I evaluated the resulting model in the same way I evaluate the final one below.

The code below shows the final tuning parameters I chose as well as the caret code that performs the search. The seed of 6 ensures this code produces the same results every time.

```{r train model, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}

# set seed to reproduce the training/test splits used for cross validation
set.seed(6)

# establish grid of parameters for xgboost algorithm
parameter_grid <- expand.grid(
  nrounds = c(25, 50, 100), 
  eta = c(0, .01, 0.1),
  max_depth = c(5, 10, 15, 20),
  gamma = c(0, 1, 2, 5),
  colsample_bytree = c(0.1, 0.5, 0.9),
  min_child_weight = c(0, 0.1),
  subsample = c(0.1, 0.2, 0.5, 0.7))

# establish 10-fold cross-validation and other training characteristics
control_parameters <- trainControl(method = "cv",
                                   number = 10,
                                   search = "grid",
                                   allowParallel = TRUE)

#find the best model using characteristics and tuning parameters above
second_half_k_predictor <- train(`2ndHalfK%` ~ .,
                                 data = pitchers_2017_train,
                                 method = "xgbTree",
                                 trControl = control_parameters,
                                 tuneGrid = parameter_grid,
                                 metric = "RMSE")
```

You can see the chosen xgboost parameters by running `second_half_k_predictor$bestTune`.

## Predict Second-Half Strikeout Rates
Next I use the model to predict the second-half K% for each pitcher. The actual predictions are written to a .csv file for ease of distribution. The rest of this article focuses on evaluating the model itself.

```{r predict results, echo=TRUE, message=FALSE, warning=FALSE}

# predict second-half k% using model
# transform to tibble for binding to original dataset
# rename variable for clarity
# compute residuals
second_half_pitcher_predictions <-
  predict(second_half_k_predictor, newdata = pitchers_2017_train) %>% 
  as_tibble() %>%
  select(`pred_2ndHalfK%` = value)

# augment original dataset with predictions
pitchers_2017_predicted <- 
  pitchers_2017 %>%
  bind_cols(second_half_pitcher_predictions)

# select actual values, predicted values, and compute residuals for model analysis purposes
results <- pitchers_2017_predicted %>%
  select(Name,
         predicted_second_half_k_rate = `pred_2ndHalfK%`,
         actual_second_half_k_rate = `2ndHalfK%`) %>%
  mutate(residual = actual_second_half_k_rate - predicted_second_half_k_rate)

# write results to CSV file for distribution. round predictions to 3 decimals for easier reading
results %>%
  select(-residual) %>%
  mutate(predicted_second_half_k_rate = round(predicted_second_half_k_rate, 3)) %>%
  write_csv("second-half-k-rate-predictions.csv")
```

## Evaluate Results

I first plot the predicted vs. actual values, visually inspect the scatterplot, and see the R^2 of the line of best fit. The following plot shows this information:

```{r plot results, echo=TRUE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
# calculate R^2 to see how much variance the model explains
model_fit <- lm(results$actual_second_half_k_rate ~ results$predicted_second_half_k_rate)
r_squared <- round(summary(model_fit)$r.squared, 3)

# plot predicted vs. actual values
results %>%  
  ggplot(aes(predicted_second_half_k_rate, actual_second_half_k_rate)) + geom_point() + fgt +
  labs(x="Predicted Second-Half K%", y="Actual Second-Half K%") +
  ggtitle("2017 Second-Half K%: Predicted Rates vs. Actual Rates") +
  xlim(0, 0.5) + ylim(0, 0.5) + geom_smooth(method="lm") + 
  geom_label(aes(x = 0.375, y = 0.45), label = sprintf("R^2: %s", r_squared)) +
  geom_abline(yintercept = 0, linetype = "dashed")
```

Two lines are shown. The dashed reference line shows a hypothetical perfect match between all model predictions actual results. The blue smoothed line shows the actual match between the model's predictions and actual results.

Visual inspection shows that these lines are reasonably close together and that the actual points on the plot resemble a line. Therefore the R^2 of the best-fit line, 0.962, is an appropriate way to measure the explanatory power of the model.

The next plot shows the predicted values vs. the residuals of each predicted value, as well as a dashed reference line and the RMSE of the predictions:

```{r plot residuals, echo=TRUE, fig.align='center'}
# calculate RMSE
rmse <- round(
  RMSE(results$predicted_second_half_k_rate, results$actual_second_half_k_rate),
  3)

# plot fitted vs. residuals to identify bias
results %>%
  ggplot(aes(predicted_second_half_k_rate, residual)) + geom_point() + fgt +
  labs(x="Predicted Second-Half K%", y="Residual", caption = sprintf("RMSE: %s", rmse))+
  ggtitle("2017 Second-Half K%: Predicted Rates vs. Residuals") +
  xlim(0, .5) + geom_hline(yintercept = 0, linetype = "dashed")
```

The plot shows some skew to the residuals, but not an unreasonable amount. The bulk of the misses, and the largest ones, are negative, which means the model tends to over-predict second-half K%. GM's should consider this information in deciding whether to acquire, trade, or keep a pitcher based on the prediction.

The RMSE of the predictions is 0.015, or 1.5 percentage points of second-half strikeout rate. Out of all the models I generated, with all the dataset and tuning parameters combinations I tried, this value is the lowest.

The final code chunk saves the model to a file so it can be used to reproduce results later.

```{r show results, echo=TRUE, message=FALSE, warning=FALSE}
# save model to a file so it can be used later.
saveRDS(second_half_k_predictor, "phillies-k-rate-predictor.rds")
```

# Areas for Improvement
For a future project I would analyze more modeling types besides xgboost. It could be that other types perform better. I'd also like to minimize the residuals and see them distributed more randomly. This might involve searching different tuning parameter spaces or including more, or different kinds of, data points. For example, knowing a team's second-half schedule could help predict future strikeout rates. Additionally, [park factors](https://www.fangraphs.com/guts.aspx?type=pf&teamid=0&season=2017) influence strikeout rates. 

Finally, I'd like to incorporate probability distributions and credible intervals instead of point estimates. Seeing a distribution could convince a GM to move on a certain player in a way that a single estimate might not.

# Conclusion
All models can be improved. But based on the 0.962 R^2, the reasonable-looking plot of the residuals, and the low RMSE of 0.015, this model is an excellent base from which to start making predictions. 