---
title: "Predicting K% in the Second Half of 2017"
author: "Ryan Pollack"
date: "October 2018"
output: html_document
---

# What is this Analysis?
This project predicts second-half strikeout rates for a select group of pitchers in 2017. It's in response to the questionnaire located at https://questionnaire-148920.appspot.com/qa/.

# Why is it Important?
Going into the second half of each season, GM's have to decide whom to target in trade, whom to put onto their own trading block, and whom to keep on the roster. Predicting second-half strikeout rates can aid all of these decisions by identifying pitchers whom other teams undervalue and therefore should be sought in a trade. as well as pitchers whom other teams overvalue and therefore should be offered up in trade. Finally, predicting second-half strikeout rates can help GM's identify pitchers who are in a good position to help the team in the current season and therefore should be kept on the roster.

# Success Metrics
How will we know these predictions are successful? I'll evaluate the predictions in four ways: the scatterplot of predicted vs. actual values, the R^2 of the best-fit line on this scatterplot, the scatterplot of the predicted values vs. residuals, and the RMSE of the predicted vs. actual values.    

# Analysis

The following sections are the R code that set up the analysis, get the data, train the model, predict results, and evaluate the predictions.

## Setup

The following code sets up the libraries and ggplot themes used:

```{r setup, echo = TRUE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# load required libraries
library(tidyverse)
library(xgboost)
library(caret)

# define plot aesthetics
fgt = theme(
  panel.background = element_rect(color = "lightgrey", fill = "white"),
  axis.title = element_text(family = "Lato"),
  axis.text = element_text(family = "Lato"),
  legend.title = element_text(family = "Lato"),
  legend.text = element_text(family = "Lato"),
  legend.background = element_rect(fill = "white"),
  legend.key = element_rect(fill = "white"),
  plot.title = element_text(family = "Lato"),
  panel.grid.major = element_line(size = .1, color = "lightgrey"),
  panel.grid.minor = element_line(size = 0),
  strip.text = element_text(family="Lato")
)

```

## Get Data
In addition to the data contained in strikeouts.csv, I incorporated the throwing hand of each pitcher. I also included the pitcher's strikeout rate in 2016. Strikeout rate is among the most stable of pitching metrics; it correlates year-to-year very well. Therefore it stands to reason that it would be informative in predicting a pitcher's K% in the second half of a season.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# get data from the first half of 2017
# source: https://questionnaire-148920.appspot.com/qa/
pitcher_data <- read_csv("~/Downloads/strikeouts.csv")

# get throwing hand of pitchers. source: fangraphs
pitcher_handedness <- read_csv("pitcher_handedness.csv")

# get strikeout rates from 2016. source: fangraphs
pitcher_data_2016 <- read_csv("2016-strikeout-rates.csv")

# combine datasets into one tibble
pitchers_2017 <- inner_join(pitcher_data, pitcher_handedness, by=c("fangraphs_id" = "PlayerId")) %>%
  inner_join(pitcher_data_2016, by=c("fangraphs_id" = "PlayerId"))

# create training dataset by removing unnecessary features
pitchers_2017_train <- 
  pitchers_2017 %>% 
  select(-fangraphs_id, -`2ndHalfIP`, -Name, -Team)
```

## Choose the Algorithm
I chose the xgboost algorithm because of its reputation as a fast, accurate algorithm that handles collinearity well. This latter feature means I can focus less about which features to include and focus more on tuning the model.

For example, FB% and GB% are mutually exclusive and so are strongly correlated (r = -0.92). With another algorithm I'd have to choose one or the other. The xgboost algorithm helps ensure I can include both without worrying about overfitting. 

For reference, the following figure shows a correlation plot of all features I used except for throwing hand:

```{r correlation matrix, echo=TRUE, fig.align='center'}

# compute and display correlation matrix for features.
# remove features we don't care about, compute its correlation matrix, add in row names because
# converting the matrix to a tibble removes them, tidy, and plot
pitchers_2017_train %>%
  select(-Throws, -`2ndHalfK%`) %>%
  cor() %>%
  as_tibble() %>%
  bind_cols(pitchers_2017_train %>% 
              select(-Throws, -`2ndHalfK%`) %>%
              names() %>%
              as_tibble()) %>%
  select(metric = value, everything()) %>%
  gather(other_metric, r, -metric) %>%
  ggplot(aes(metric, other_metric, color = r, fill = r)) + geom_tile() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(x="", y="") + ggtitle("Correlation Matrix of Features")
```

## Tune and Train the Model
I used the caret package to tune and train the model. I defined a set of tuning parameters for the xgboost algorithm itself, performed 10-fold cross-validation for each combination of parameters, and selected the model that minimized the RMSE of the predictions. The caret package also handles encoding the categorical "Throws" feature. 

This process is iterative in itself, but choosing the parameters for `parameter_grid` is iterative too. The `nrounds` (the number of decision trees to use) and `max_depth` (how deep each tree can go) parameters are important, so I experiemented with several option sets for each. After each training run I evaluated the resulting model in the same way I evaluate the final one below.

The code below shows the final tuning parameters I chose as well as the caret code that performs the search. The random seed of 6 ensures this code produces the same results every time.


```{r train model, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}

# set seed to reproduce the training/test splits used for cross validation
set.seed(6)

# establish grid of parameters for xgboost algorithm
parameter_grid <- expand.grid(
  nrounds = c(25, 50, 100), 
  eta = c(0, .01, 0.1),
  max_depth = c(5, 10, 15, 20),
  gamma = c(0, 1, 2, 5),
  colsample_bytree = c(0.1, 0.5, 0.9),
  min_child_weight = c(0, 0.1),
  subsample = c(0.1, 0.2, 0.5, 0.7))

# establish 10-fold cross-validation and other training characteristics
control_parameters <- trainControl(method = "cv",
                                   number = 10,
                                   search = "grid",
                                   allowParallel = TRUE)

#find the best model using characteristics and tuning parameters above
second_half_k_predictor <- train(`2ndHalfK%` ~ .,
                                 data = pitchers_2017_train,
                                 method = "xgbTree",
                                 trControl = control_parameters,
                                 tuneGrid = parameter_grid,
                                 metric = "RMSE")
```

You can see the chosen parameters by running `second_half_k_predictor$bestTune`.

## Predict Second-Half Strikeout Rates
The next code chunk uses the model to predict the second-half K% for each pitcher. 

```{r predict results, echo=TRUE, message=FALSE, warning=FALSE}

# predict second-half k% using model
# transform to tibble for binding to original dataset
# rename variable for clarity
second_half_pitcher_predictions <-
  predict(second_half_k_predictor, newdata = pitchers_2017_train) %>% 
  as_tibble() %>%
  select(`pred_2ndHalfK%` = value)

# augment original dataset with predictions
pitchers_2017_predicted <- 
  pitchers_2017 %>%
  bind_cols(second_half_pitcher_predictions)

# calculate actual values, predicted values, and residuals for model analysis purposes
results <- pitchers_2017_predicted %>%
  select(Name,
         predicted_second_half_k_rate = `pred_2ndHalfK%`,
         actual_second_half_k_rate = `2ndHalfK%`) %>%
  mutate(residual = actual_second_half_k_rate - predicted_second_half_k_rate)
```

## Evaluate Results

I evaluate the model in two ways. I first plot the predicted vs. actual values, visually inspect the scatterplot, and see the R^2 of the line of best fit. The following plot shows this information:

```{r plot results, echo=TRUE, fig.align='center', message=FALSE, warning=FALSE, paged.print=FALSE}
# calculate R^2 to see how much variance the model explains
model_fit <- lm(results$actual_second_half_k_rate ~ results$predicted_second_half_k_rate)
r_squared <- round(summary(model_fit)$r.squared, 3)

# plot predicted vs. actual values
results %>%  
  ggplot(aes(predicted_second_half_k_rate, actual_second_half_k_rate)) + geom_point() + fgt +
  labs(x="Predicted Second-Half K%", y="Actual Second-Half K%") +
  ggtitle("2017 Second-Half K%: Predicted Rates vs. Actual Rates") +
  xlim(0, 0.5) + ylim(0, 0.5) + geom_smooth(method="lm") + 
  geom_label(aes(x = 0.425, y = 0.35), label = sprintf("R^2: %s", r_squared))
```

The scatterplot shows the tight fit between the model's predicted results and the actual results. The R^2 of this fit shows that the model explains 96% of the variance in the actual second-half K rates.

The next plot shows the predicted values vs. the residuals of each predicted value, as well as the RMSE of the predictions:

```{r plot residuals, echo=TRUE, fig.align='center'}
# calculate RMSE
rmse <- round(
  RMSE(results$predicted_second_half_k_rate, results$actual_second_half_k_rate),
  3)

# plot fitted vs. residuals to identify bias
results %>%
  ggplot(aes(predicted_second_half_k_rate, residual)) + geom_point() + fgt +
  labs(x="Predicted Second-Half K%", y="Residual", caption = sprintf("RMSE: %s", rmse))+
  ggtitle("2017 Second-Half K%: Predicted Rates vs. Residuals") +
  xlim(0, .5)
```

The plot shows some skew to the residuals, but not an unreasonable amount. The RMSE of the predictions is 0.015, or 1.5 percentage points of second-half strikeout rate.

The final code chunk creates a CSV file of predicted and actual second-half K rates for each pitcher, for ease of distribution. It also saves the model itself to a file so it can be used to reproduce results later.

```{r show results, echo=TRUE, message=FALSE, warning=FALSE}
# write results to CSV file for distribution. round predictions to 3 decimals for easier reading
results %>%
  select(-residual) %>%
  mutate(predicted_second_half_k_rate = round(predicted_second_half_k_rate, 3)) %>%
  write_csv("k-rate-predictions.csv")

# save model to a file so it can be used later.
saveRDS(second_half_k_predictor, "phillies-k-rate-predictor.rds")
```

# Conclusion
All models can be improved. But based on the 0.962 R^2, the reasonable-looking plot of the residuals, and the low RMSE of 0.015, this model is an excellent base from which to start making predictions. 