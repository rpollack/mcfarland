---
title: "Predicting K% in the Second Half of 2017"
author: "Ryan Pollack"
date: "October 2018"
output: html_document
---

# What is this Analysis?
This project predicts second-half strikeout rates for a select group of pitchers in 2017. It's in response to the questionnaire located at https://questionnaire-148920.appspot.com/qa/.

# Why is it Important?
Going into the second half of each season, GM's have to decide whom to target in trade, whom to put onto their own trading block, and whom to keep on the roster. Predicting second-half strikeout rates can aid all of these decisions by identifying pitchers whom other teams might undervalue and therefore should be sought in a trade, whom other teams might overvalue and therefore should be offered up in trade, and who can help the current roster succeed.

# Success Metrics
How will we know these predictions are successful? I'll evaluate the predictions in four ways: the scatterplot of predicted vs. actual values, the R^2 of the best-fit line on this scatterplot, the scatterplot of the predicted values vs. residuals, and the RMSE of the predicted vs. actual values.

This project uses RMSE instead of mean absolute error (MAE) because RMSE penalizes large errors more than MAE does. In the world of baseball where roster spots are limited, and especially in forecasting second-half results in order to drive trade decisions, large misses in predicted K% can have outsized effects.

# Analysis
The following sections are the R code that set up the analysis, get the data, train the model, predict results, and evaluate the predictions.

## Setup
The following code sets up the libraries and ggplot themes used:

```{r setup, echo = TRUE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
# load required libraries
library(tidyverse)
library(xgboost)
library(caret)
library(broom)

# define plot aesthetics
fgt = theme(
  panel.background = element_rect(color = "lightgrey", fill = "white"),
  axis.title = element_text(family = "Lato"),
  axis.text = element_text(family = "Lato"),
  legend.title = element_text(family = "Lato"),
  legend.text = element_text(family = "Lato"),
  legend.background = element_rect(fill = "white"),
  legend.key = element_rect(fill = "white"),
  plot.title = element_text(family = "Lato"),
  panel.grid.major = element_line(size = .1, color = "lightgrey"),
  panel.grid.minor = element_line(size = 0),
  strip.text = element_text(family="Lato")
)

```

## Data Acquisition and Processing
I used all the data in strikeouts.csv except G, IP, GB%, FB%, and LD%. I found during the course of selecting and evaluating models that including these features reduced the predictive abilities of the models.

I left FIP and AVG in the dataset even though they're highly correlated with K% at r > 0.75. Removing these columns reduced the R^2 and increased the RMSE of all model types.

In addition to the data that remained, I incorporated the throwing hand of each pitcher. So much of batter/pitcher matchups involves platoon splits that I wanted to see if that information helped the model. I also included the pitcher's strikeout rate in 2016. Strikeout rate is among the most stable of pitching metrics; it correlates year-to-year very well, so I thought it might help second-half 2017 predictions. 

Both additional datasets proved useful in constructing the model.

```{r}
# get data from the first half of 2017
# source: https://questionnaire-148920.appspot.com/qa/
pitcher_data <- read_csv("~/Downloads/strikeouts.csv") %>%
  select(-G, -IP, -`GB%`, -`FB%`, -`LD%`)

# not run: for demonstration purposes: identify predictors correlated at above 0.75
# pitchers_2017 %>% 
#   select(-Name, -Team, -fangraphs_id, -Throws, -contains("2ndHalf")) %>% 
#   cor() %>% 
#   findCorrelation(cutoff = 0.75, names = TRUE)

# get throwing hand of pitchers. source: fangraphs
pitcher_handedness <- read_csv("pitcher_handedness.csv")

# get strikeout rates from 2016. source: fangraphs
pitcher_data_2016 <- read_csv("2016-strikeout-rates.csv")

# combine datasets into one tibble
pitchers_2017 <- inner_join(pitcher_data, pitcher_handedness, by=c("fangraphs_id" = "PlayerId")) %>%
  inner_join(pitcher_data_2016, by=c("fangraphs_id" = "PlayerId"))
```

## Create Training/Validation and Test Datasets

The following code removes unimportant variables from the dataset and defines 75% of it for training/validation and 25% of it for testing/model evaluation. This latter dataset is held out from training and validation, making it suitable for model evaluation.

```{r create train/validate and test datasets, message=FALSE, warning=FALSE}
# create dataset from which test & validation/train splits will be made

seed <- 2356

set.seed(seed)

partition_df <-
  pitchers_2017 %>%
  select(-Name, -Team, -fangraphs_id, -`2ndHalfIP`)

# get rows to be used for test dataset: 25% of full dataset's rows
test_df_rows <- 
  createDataPartition(partition_df$`2ndHalfK%`, p = 0.25) %>%
  as_tibble()

# create test dataset
test_df <- 
  partition_df %>%
  filter(row_number() %in% test_df_rows$Resample1)

# use remaining 75% of rows for training & validation
train_df <- 
  partition_df %>%
  filter(!row_number() %in% test_df_rows$Resample1)

```

## Model Training and Validation

I used the caret package with the training/validation dataset to find the optimal parameter values for xgboost, neural network, support vector machine, k nearest neighbors, and linear model types. Choosing a variety of nonlinear models as well as a linear one helps ensure we find the model that provides the best possible fit.

I train and validate each model by using 10-fold cross-validation repeated 5 times. Each training/validation run attempts to minimize the RMSE metric of the resulting model.

```{r train and validate models, message=FALSE, warning=FALSE}

# establish training parameters
control_parameters <- trainControl(method = "repeatedcv",
                                   repeats = 5,
                                   number = 10,
                                   allowParallel = TRUE,
                                   verboseIter = FALSE)

# train all model types
set.seed(seed)
second_half_k_predictor_xgb <- train(`2ndHalfK%` ~ .,
                                     data = train_df,
                                     method = "xgbTree",
                                     trControl = control_parameters,
                                     metric = "RMSE")

set.seed(seed)
second_half_k_predictor_nnet <- train(`2ndHalfK%` ~ .,
                                      data = train_df,
                                      method = "nnet",
                                      trControl = control_parameters,
                                      metric = "RMSE",
                                      trace = FALSE)

set.seed(seed)
second_half_k_predictor_svm <- train(`2ndHalfK%` ~ .,
                                     data = train_df,
                                     method = "svmRadial",
                                     trControl = control_parameters,
                                     metric = "RMSE")

set.seed(seed)
second_half_k_predictor_knn <- train(`2ndHalfK%` ~ .,
                                     data = train_df,
                                     method = "knn",
                                     trControl = control_parameters,
                                     metric = "RMSE")

set.seed(seed)
second_half_k_predictor_lm <- train(`2ndHalfK%` ~ .,
                                     data = train_df,
                                     method = "glm",
                                     trControl = control_parameters,
                                     metric = "RMSE")
```

## Model Testing and Evaluation

The code below evaluates each model type by predicting the second-half K% in the test/out-of-sample dataset and displaying the success metrics I listed earlier in this report. 

```{r model testing and evaluation, message=FALSE, warning=FALSE}

# using data held out from training and validation, predict second-half K% using each model type
predictions_xgb <-
  predict(second_half_k_predictor_xgb, newdata = test_df) %>% 
  as_tibble() %>%
  select(xgb = value)

predictions_nnet <-
  predict(second_half_k_predictor_nnet, newdata = test_df) %>% 
  as_tibble() %>%
  select(nnet = V1)

predictions_svm <-
  predict(second_half_k_predictor_svm, newdata = test_df) %>% 
  as_tibble() %>%
  select(svm = value)

predictions_knn <-
  predict(second_half_k_predictor_knn, newdata = test_df) %>% 
  as_tibble() %>%
  select(knn = value)

predictions_lm <-
  predict(second_half_k_predictor_lm, newdata = test_df) %>% 
  as_tibble() %>%
  select(linear = value)

# combine all models' predictions into one tibble, tidy, and compute residuals
model_evaluation <- 
  test_df %>%
  bind_cols(predictions_xgb, predictions_knn, predictions_svm, predictions_nnet, predictions_lm) %>%
  select(actual_second_half_k_rate = `2ndHalfK%`, xgb, svm, nnet, knn, linear) %>%
  gather(model_type, prediction, -actual_second_half_k_rate) %>%
  mutate(residual = actual_second_half_k_rate - prediction)

# display scatterplots, lines of best-fit, graphs of residuals, R^2, and RMSE for each model type
model_evaluation %>% 
  ggplot(aes(actual_second_half_k_rate, prediction)) + geom_point() +
  facet_wrap(vars(model_type)) + geom_smooth(method="lm") +
  labs(x="Actual 2nd Half K%", y = "Predicted 2nd Half K%") + fgt + expand_limits(x=0,y=0) +
  geom_abline(yintercept = 0, linetype = "dashed") + ggtitle("Best-fit Lines for Each Model Type", 
                                                             subtitle = "Using out-of-sample data")

model_evaluation %>% 
  ggplot(aes(prediction, residual)) + geom_point() +
  facet_wrap(vars(model_type)) + 
  labs(x = "Predicted 2nd Half K%", y="Residual") + fgt +
  ggtitle("Residual Plots for Each Model Type", subtitle = "Using out-of-sample data") +
  geom_hline(yintercept = 0, linetype = "dashed") 

r_squared <- model_evaluation %>%
  split(.$model_type) %>%
  map(~lm(prediction ~ actual_second_half_k_rate, data = .)) %>%
  map(summary) %>%
  map_dbl("r.squared") %>%
  tidy() %>% 
  select(model_type = names, r_squared = x)

# display table of model evaluation statistics  
model_evaluation %>%
  group_by(model_type) %>%
  summarize(RMSE = RMSE(prediction, actual_second_half_k_rate)) %>%
  inner_join(r_squared, by="model_type") %>%
  select(`Model Type` = model_type, `R^2` = r_squared, RMSE)
```

In the plots above, the dashed lines are reference lines that show what a perfect fit would look like.

## Choosing the Best Model

I chose the linear model for several reasons. It has the highest R^2 and the lowest RMSE. Its scatterplot contains no outliers that would cause the stated R^2 to lose meaning (like in Anscombe's Quartet) and its residuals graph is closest to a horizontal line around y=0. I'm confident in this evaluation because it was done on out-of-sample data.

## Predicting Second-Half K% 

Now that we have a model, the following code uses it to predict the second-half K% for each pitcher. The predictions are written to a CSV file for convenience.

```{r final results, message=FALSE, warning=FALSE}
final_results <-
  partition_df %>%
  mutate(predicted_second_half_k_rate = predict(second_half_k_predictor_lm, newdata = .)) %>%
  bind_cols(pitchers_2017 %>% select(Name)) %>%
  select(Name, actual_second_half_k_rate = `2ndHalfK%`, predicted_second_half_k_rate)

final_results %>%
  write_csv("phillies-second-half-predicted-k-rates.csv")
```

# Areas for Improvement

In the future I'd most like to build a generalized K% predictor. The request here was for one that predicting K% in a specific timeframe; a more useful program would do something like take a requested prediction year and timeframe, build an appropriate model, and make the predictions. This predictor could be further generalized to any stat.

I would analyze even more modeling types besides the ones listed here. I don't yet have enough experience to understand which model type(s) may best fit this kind of predcition. But this report shows my general philosophy of training many models, evaluating them using out-of-sample data, and using the best-performing model to make predictions. The caret package is of tremendous value here.

I'd like to better separate the data I used. While the model evaluation was done on out-of-sample data, the actual predictions were done using some data that trained the model. I'd prefer to use first-half/second-half data from, say, 2014-2016 to build the model, then apply that model to first-half stats in 2017. I was unable to find first-half plate discipline data on FanGraphs, however.

Finally, I'd like to incorporate probability distributions and credible intervals instead of point estimates. Seeing a distribution could convince a GM to move on a certain player in a way that a single estimate might not. And talking about credible intervals and probabilities helps show non-technical stakeholders that you understand prediction is an inexact science.